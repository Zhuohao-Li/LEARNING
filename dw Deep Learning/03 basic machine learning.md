# notes3 basic of machine learning

1. æ•°æ®é›†
2. è¯¯å·®åˆ†æ
3. ä»£è¡¨çš„æœºå™¨å­¦ä¹ æ–¹æ³•
   1. æœ‰ç›‘ç£ã€çº¿æ€§å›å½’ã€SVMã€å†³ç­–æ ‘ã€RF
   2. æ— ç›‘ç£ã€èšç±»ã€é™ç»´ï¼ˆPCAï¼‰

# machine learning

### concept

##### category

- SupervisedLearning ( Category, Regression)
- UnsupervisedLearning (Converge, decrease dimension)
- Reinforcement Learning

### dataset

like this, $ğ·={ğ‘¥_1,ğ‘¥_2,â‹¯,ğ‘¥_ğ‘›}$ includes _n_ samplesï¼Œ $ğ‘¥_ğ‘–$ is a vectorï¼Œwhich shows i sample in the dataset. ğ‘‘ is the space dimension

**category of dataset**

- Trainingset
- Validation set
- Testset
  **classic dataset**

- å›¾åƒåˆ†ç±»
  - MNIST http://yann.lecun.com/exdb/mnist/
  - CIFAR-10, CIFAR-100, ImageNet
    - https://www.cs.toronto.edu/~kriz/cifar.html
    - http://www.image-net.org/
  - Large Movie Review Dataset v1.0
    - http://ai.stanford.edu/~amaas/data/sentiment/
  - æ•°æ®é›†:https://github.com/researchmm/img2poem

# error analysis

**over fitting**ç­‰

**lack fitting**

### general error analysis

$$
\begin{array}{l}\operatorname{Err}(\hat{f})=\mathrm{E}\left[(Y-\hat{f}(\mathrm{X}))^{2}\right] \\ \operatorname{Err}(\hat{f})=\mathrm{E}\left[(f(X)+\varepsilon-\hat{f}(\mathrm{X}))^{2}\right] \\ \operatorname{Err}(\hat{f})=\mathrm{E}\left[(f(X)-\hat{f}(\mathrm{X}))^{2}+2 \varepsilon(f(X)-\hat{f}(\mathrm{X}))+\varepsilon^{2}\right] \\ \operatorname{Err}(\hat{f})=\mathrm{E}\left[(E(\hat{f}(\mathrm{X}))-f(X)+\hat{f}(\mathrm{X})-E(\hat{f}(\mathrm{X})))^{2}\right]+\sigma_{\varepsilon}^{2} \\ \operatorname{Err}(\hat{f})=\mathrm{E}[(E(\hat{f}(\mathrm{X}))-f(X))]^{2}+\mathrm{E}\left[(\hat{f}(\mathrm{X})-E(\hat{f}(\mathrm{X})))^{2}\right]+\sigma_{\varepsilon}^{2} \\ \operatorname{Err}(\hat{f})=\operatorname{Bias}^{2}(\hat{f})+\operatorname{Var}(\hat{f})+\sigma_{\varepsilon}^{2}\end{array}
$$

bias and variance

### cross verification

# supervised

- æ•°æ®é›†æœ‰æ ‡è®°(ç­”æ¡ˆ)
- æ•°æ®é›†é€šå¸¸æ‰©å±•ä¸º$(ğ‘¥_ğ‘–,ğ‘¦_ğ‘–)$ï¼Œå…¶ä¸­$ğ‘¦_ğ‘–âˆˆY$æ˜¯ $ğ‘¥_ğ‘–$ çš„æ ‡è®°ï¼Œ$Y$ æ˜¯æ‰€æœ‰æ ‡è®°çš„é›†åˆï¼Œç§°ä¸ºâ€œæ ‡è®°ç©ºé—´â€æˆ–â€œè¾“å‡ºç©ºé—´â€
- ç›‘ç£å­¦ä¹ çš„ä»»åŠ¡æ˜¯è®­ç»ƒå‡ºä¸€ä¸ªæ¨¡å‹ç”¨äºé¢„æµ‹ $ğ‘¦$ çš„å–å€¼ï¼Œæ ¹æ® $ğ·=\{(ğ‘¥_1,ğ‘¦_1 ),(ğ‘¥_2,ğ‘¦_2),â‹¯, (ğ‘¥_ğ‘›,ğ‘¦_ğ‘›)\}$ï¼Œè®­ç»ƒå‡ºå‡½æ•° ğ‘“ï¼Œä½¿å¾—$ğ‘“(ğ‘¥)â‰…ğ‘¦$
- è‹¥é¢„æµ‹çš„å€¼æ˜¯ç¦»æ•£å€¼ï¼Œå¦‚å¹´é¾„ï¼Œæ­¤ç±»å­¦ä¹ ä»»åŠ¡ç§°ä¸ºâ€œåˆ†ç±»â€
- è‹¥é¢„æµ‹çš„å€¼æ˜¯è¿ç»­å€¼ï¼Œå¦‚æˆ¿ä»·ï¼Œæ­¤ç±»å­¦ä¹ ä»»åŠ¡ç§°ä¸ºâ€œå›å½’â€

### linear regression

$$
f(x^k) = w_1x_1^k+w_2x_2^k+\cdots+w_mx_m^k+b = \sum_{i=1}^m w_ix_i^k+b
$$

$$
(w^*,b^*) = argmin_{(w,b)}\sum_{k = 1}^n(f(x^k)-y^k)^2 = argmin_{(w,b)}\sum_{k = 1}^n(w^Tx^k+b-y^k)^2
$$

### logistic regression

$$
g(f(x^k))=
\left\{\begin{array}{l}
1, \frac{1}{1+e^{-(w^Tx^k+b)}}\geq 0.5 \\ 0,  otherwise
\end{array}\right.
$$

### support SVM

$$
\begin{align}
x &= x_0 + \gamma \frac{w}{\|w\|}
\\
\gamma &= \frac{w^Tx + b}{\|w\|} = \frac{f(x)}{w}
\end{align}
$$

$$
\arg \max_{w,b} \arg \min_{x_i \in D} \frac{|w^Tx_i+b|}{\sqrt{\sum_{i = 1}^dw_i^2}} \\s.t. \forall x_i \in D,y_i(w^Tx_i+b)\geq 0
$$

$\forall x_i \in D,|w^Tx_i+b| \geq 1$.

$$
\arg \min_{w,b}  \frac{1}{2}\sum_{i = 1}^d w_i^2\\s.t. \forall x_i \in D,|w^Tx_i+b| \geq 1
$$

### decision making tree

Based on data structure Tree

# unsupervised

### converage

classic algorithm: K-means

### lower dimension
